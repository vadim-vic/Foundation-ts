
@misc{noauthor_register_nodate,
	title = {Register},
	url = {https://www.zotero.org/user/register},
	urldate = {2025-07-02},
	file = {Register:/Users/victor/Zotero/storage/LX3XQ943/register.html:text/html},
}

@misc{hu_time-ssm_2024,
	title = {Time-{SSM}: {Simplifying} and {Unifying} {State} {Space} {Models} for {Time} {Series} {Forecasting}},
	shorttitle = {Time-{SSM}},
	url = {http://arxiv.org/abs/2405.16312},
	doi = {10.48550/arXiv.2405.16312},
	abstract = {State Space Models (SSMs) have emerged as a potent tool in sequence modeling tasks in recent years. These models approximate continuous systems using a set of basis functions and discretize them to handle input data, making them well-suited for modeling time series data collected at specific frequencies from continuous systems. Despite its potential, the application of SSMs in time series forecasting remains underexplored, with most existing models treating SSMs as a black box for capturing temporal or channel dependencies. To address this gap, this paper proposes a novel theoretical framework termed Dynamic Spectral Operator, offering more intuitive and general guidance on applying SSMs to time series data. Building upon our theory, we introduce Time-SSM, a novel SSM-based foundation model with only one-seventh of the parameters compared to Mamba. Various experiments validate both our theoretical framework and the superior performance of Time-SSM.},
	urldate = {2025-07-02},
	publisher = {arXiv},
	author = {Hu, Jiaxi and Lan, Disen and Zhou, Ziyu and Wen, Qingsong and Liang, Yuxuan},
	month = jul,
	year = {2024},
	note = {arXiv:2405.16312 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, SSM},
	annote = {Comment: arXiv admin note: text overlap with arXiv:2402.11463},
	file = {Full Text PDF:/Users/victor/Zotero/storage/7SN6X4WT/Hu et al. - 2024 - Time-SSM Simplifying and Unifying State Space Models for Time Series Forecasting.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/T7H2LFXI/2405.html:text/html},
}

@book{haller_modeling_2025,
	address = {Philadelphia, PA},
	title = {Modeling {Nonlinear} {Dynamics} from {Equations} and {Data} — with {Applications} to {Solids}, {Fluids}, and {Controls}},
	isbn = {978-1-61197-834-6 978-1-61197-835-3},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611978353},
	language = {en},
	urldate = {2025-07-02},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Haller, George},
	month = jan,
	year = {2025},
	doi = {10.1137/1.9781611978353},
	keywords = {SSM},
}

@misc{hallergroupeth_nonlinear_2024,
	title = {Nonlinear {Reduced}-{Order} {Modeling} from {Data} by {Prof}. {George} {Haller}.},
	url = {https://www.youtube.com/watch?v=mhcZaBMeA-U},
	abstract = {NODY Webinar, February 22, 2024.
DOI: https://doi.org/10.52843/cassyni.jrr0qt},
	urldate = {2025-07-02},
	author = {{HallerGroupETH}},
	month = feb,
	year = {2024},
}

@book{noauthor_nonparametric_2006,
	series = {Springer {Series} in {Statistics}},
	title = {Nonparametric {Functional} {Data} {Analysis}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-0-387-30369-7},
	url = {http://link.springer.com/10.1007/0-387-36620-2},
	language = {en},
	urldate = {2025-07-04},
	publisher = {Springer New York},
	year = {2006},
	doi = {10.1007/0-387-36620-2},
	keywords = {calculus, data analysis, econometrics, modeling, nonparametric methods, Parametric statistics, Pattern Recognition, statistical method, statistics},
}

@article{papaioannou_time_2022,
	title = {Time {Series} {Forecasting} {Using} {Manifold} {Learning}},
	volume = {32},
	issn = {1054-1500, 1089-7682},
	url = {http://arxiv.org/abs/2110.03625},
	doi = {10.1063/5.0094887},
	abstract = {We address a three-tier numerical framework based on manifold learning for the forecasting of high-dimensional time series. At the first step, we embed the time series into a reduced low-dimensional space using a nonlinear manifold learning algorithm such as Locally Linear Embedding and Diffusion Maps. At the second step, we construct reduced-order regression models on the manifold, in particular Multivariate Autoregressive (MVAR) and Gaussian Process Regression (GPR) models, to forecast the embedded dynamics. At the final step, we lift the embedded time series back to the original high-dimensional space using Radial Basis Functions interpolation and Geometric Harmonics. For our illustrations, we test the forecasting performance of the proposed numerical scheme with four sets of time series: three synthetic stochastic ones resembling EEG signals produced from linear and nonlinear stochastic models with different model orders, and one real-world data set containing daily time series of 10 key foreign exchange rates (FOREX) spanning the time period 03/09/2001-29/10/2020. The forecasting performance of the proposed numerical scheme is assessed using the combinations of manifold learning, modelling and lifting approaches. We also provide a comparison with the Principal Component Analysis algorithm as well as with the naive random walk model and the MVAR and GPR models trained and implemented directly in the high-dimensional space.},
	number = {8},
	urldate = {2025-07-07},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Papaioannou, Panagiotis and Talmon, Ronen and Kevrekidis, Ioannis and Siettos, Constantinos},
	month = aug,
	year = {2022},
	note = {arXiv:2110.03625 [math]},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
	file = {Preprint PDF:/Users/victor/Zotero/storage/Z6XR9U6C/Papaioannou et al. - 2022 - Time Series Forecasting Using Manifold Learning.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/6V3HXYTY/2110.html:text/html},
}

@article{papaioannou_time-series_2022,
	title = {Time-series forecasting using manifold learning, radial basis function interpolation, and geometric harmonics},
	volume = {32},
	issn = {1054-1500},
	url = {https://doi.org/10.1063/5.0094887},
	doi = {10.1063/5.0094887},
	abstract = {We address a three-tier numerical framework based on nonlinear manifold learning for the forecasting of high-dimensional time series, relaxing the “curse of dimensionality” related to the training phase of surrogate/machine learning models. At the first step, we embed the high-dimensional time series into a reduced low-dimensional space using nonlinear manifold learning (local linear embedding and parsimonious diffusion maps). Then, we construct reduced-order surrogate models on the manifold (here, for our illustrations, we used multivariate autoregressive and Gaussian process regression models) to forecast the embedded dynamics. Finally, we solve the pre-image problem, thus lifting the embedded time series back to the original high-dimensional space using radial basis function interpolation and geometric harmonics. The proposed numerical data-driven scheme can also be applied as a reduced-order model procedure for the numerical solution/propagation of the (transient) dynamics of partial differential equations (PDEs). We assess the performance of the proposed scheme via three different families of problems: (a) the forecasting of synthetic time series generated by three simplistic linear and weakly nonlinear stochastic models resembling electroencephalography signals, (b) the prediction/propagation of the solution profiles of a linear parabolic PDE and the Brusselator model (a set of two nonlinear parabolic PDEs), and (c) the forecasting of a real-world data set containing daily time series of ten key foreign exchange rates spanning the time period 3 September 2001–29 October 2020.},
	number = {8},
	urldate = {2025-07-07},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Papaioannou, Panagiotis G. and Talmon, Ronen and Kevrekidis, Ioannis G. and Siettos, Constantinos},
	month = aug,
	year = {2022},
	pages = {083113},
	file = {Snapshot:/Users/victor/Zotero/storage/7XWB62GR/5.html:text/html;Submitted Version:/Users/victor/Zotero/storage/R7KW7B98/Papaioannou et al. - 2022 - Time-series forecasting using manifold learning, radial basis function interpolation, and geometric.pdf:application/pdf},
}

@misc{rush_srushannotated-s4_2025,
	title = {srush/annotated-s4},
	copyright = {MIT},
	url = {https://github.com/srush/annotated-s4},
	abstract = {Implementation of https://srush.github.io/annotated-s4},
	urldate = {2025-07-07},
	author = {Rush, Sasha},
	month = jul,
	year = {2025},
	note = {original-date: 2021-12-08T22:52:35Z},
	keywords = {deep-learning, jax},
}

@misc{noauthor_22_nodate,
	title = {22 - {Debiased}/{Orthogonal} {Machine} {Learning} — {Causal} {Inference} for the {Brave} and {True}},
	url = {https://matheusfacure.github.io/python-causality-handbook/22-Debiased-Orthogonal-Machine-Learning.html},
	urldate = {2025-07-07},
	file = {22 - Debiased/Orthogonal Machine Learning — Causal Inference for the Brave and True:/Users/victor/Zotero/storage/X2JZC8Q4/22-Debiased-Orthogonal-Machine-Learning.html:text/html},
}

@misc{noauthor_database_nodate,
	title = {Database - {BNCI} {Horizon} 2020},
	url = {https://bnci-horizon-2020.eu/database},
	urldate = {2025-07-07},
	file = {Database - BNCI Horizon 2020:/Users/victor/Zotero/storage/5T947MRA/database.html:text/html},
}

@inproceedings{voelker_legendre_2019,
	title = {Legendre {Memory} {Units}: {Continuous}-{Time} {Representation} in {Recurrent} {Neural} {Networks}},
	volume = {32},
	shorttitle = {Legendre {Memory} {Units}},
	url = {https://papers.nips.cc/paper/2019/hash/952285b9b7e7a1be5aa7849f32ffff05-Abstract.html},
	urldate = {2025-07-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Voelker, Aaron and Kajić, Ivana and Eliasmith, Chris},
	year = {2019},
	file = {Full Text PDF:/Users/victor/Zotero/storage/ZWY3GC46/Voelker et al. - 2019 - Legendre Memory Units Continuous-Time Representation in Recurrent Neural Networks.pdf:application/pdf},
}

@misc{ruhe_geometric_2023,
	title = {Geometric {Clifford} {Algebra} {Networks}},
	url = {http://arxiv.org/abs/2302.06594},
	doi = {10.48550/arXiv.2302.06594},
	abstract = {We propose Geometric Clifford Algebra Networks (GCANs) for modeling dynamical systems. GCANs are based on symmetry group transformations using geometric (Clifford) algebras. We first review the quintessence of modern (plane-based) geometric algebra, which builds on isometries encoded as elements of the \${\textbackslash}mathrm\{Pin\}(p,q,r)\$ group. We then propose the concept of group action layers, which linearly combine object transformations using pre-specified group actions. Together with a new activation and normalization scheme, these layers serve as adjustable \${\textbackslash}textit\{geometric templates\}\$ that can be refined via gradient descent. Theoretical advantages are strongly reflected in the modeling of three-dimensional rigid body transformations as well as large-scale fluid dynamics simulations, showing significantly improved performance over traditional methods.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Ruhe, David and Gupta, Jayesh K. and Keninck, Steven de and Welling, Max and Brandstetter, Johannes},
	month = may,
	year = {2023},
	note = {arXiv:2302.06594 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/victor/Zotero/storage/APZZ5Z54/Ruhe et al. - 2023 - Geometric Clifford Algebra Networks.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/LL9ASGNT/2302.html:text/html},
}

@article{papaioannou_time-series_2022-1,
	title = {Time-series forecasting using manifold learning, radial basis function interpolation, and geometric harmonics},
	volume = {32},
	issn = {1089-7682},
	doi = {10.1063/5.0094887},
	abstract = {We address a three-tier numerical framework based on nonlinear manifold learning for the forecasting of high-dimensional time series, relaxing the "curse of dimensionality" related to the training phase of surrogate/machine learning models. At the first step, we embed the high-dimensional time series into a reduced low-dimensional space using nonlinear manifold learning (local linear embedding and parsimonious diffusion maps). Then, we construct reduced-order surrogate models on the manifold (here, for our illustrations, we used multivariate autoregressive and Gaussian process regression models) to forecast the embedded dynamics. Finally, we solve the pre-image problem, thus lifting the embedded time series back to the original high-dimensional space using radial basis function interpolation and geometric harmonics. The proposed numerical data-driven scheme can also be applied as a reduced-order model procedure for the numerical solution/propagation of the (transient) dynamics of partial differential equations (PDEs). We assess the performance of the proposed scheme via three different families of problems: (a) the forecasting of synthetic time series generated by three simplistic linear and weakly nonlinear stochastic models resembling electroencephalography signals, (b) the prediction/propagation of the solution profiles of a linear parabolic PDE and the Brusselator model (a set of two nonlinear parabolic PDEs), and (c) the forecasting of a real-world data set containing daily time series of ten key foreign exchange rates spanning the time period 3 September 2001-29 October 2020.},
	language = {eng},
	number = {8},
	journal = {Chaos (Woodbury, N.Y.)},
	author = {Papaioannou, Panagiotis G. and Talmon, Ronen and Kevrekidis, Ioannis G. and Siettos, Constantinos},
	month = aug,
	year = {2022},
	pmid = {36049932},
	pages = {083113},
	file = {Submitted Version:/Users/victor/Zotero/storage/FD3ZQINM/Papaioannou et al. - 2022 - Time-series forecasting using manifold learning, radial basis function interpolation, and geometric.pdf:application/pdf},
}

@misc{sapienza_differentiable_2024,
	title = {Differentiable {Programming} for {Differential} {Equations}: {A} {Review}},
	shorttitle = {Differentiable {Programming} for {Differential} {Equations}},
	url = {http://arxiv.org/abs/2406.09699},
	doi = {10.48550/arXiv.2406.09699},
	abstract = {The differentiable programming paradigm is a cornerstone of modern scientific computing. It refers to numerical methods for computing the gradient of a numerical model's output. Many scientific models are based on differential equations, where differentiable programming plays a crucial role in calculating model sensitivities, inverting model parameters, and training hybrid models that combine differential equations with data-driven approaches. Furthermore, recognizing the strong synergies between inverse methods and machine learning offers the opportunity to establish a coherent framework applicable to both fields. Differentiating functions based on the numerical solution of differential equations is non-trivial. Numerous methods based on a wide variety of paradigms have been proposed in the literature, each with pros and cons specific to the type of problem investigated. Here, we provide a comprehensive review of existing techniques to compute derivatives of numerical solutions of differential equations. We first discuss the importance of gradients of solutions of differential equations in a variety of scientific domains. Second, we lay out the mathematical foundations of the various approaches and compare them with each other. Third, we cover the computational considerations and explore the solutions available in modern scientific software. Last but not least, we provide best-practices and recommendations for practitioners. We hope that this work accelerates the fusion of scientific models and data, and fosters a modern approach to scientific modelling.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Sapienza, Facundo and Bolibar, Jordi and Schäfer, Frank and Groenke, Brian and Pal, Avik and Boussange, Victor and Heimbach, Patrick and Hooker, Giles and Pérez, Fernando and Persson, Per-Olof and Rackauckas, Christopher},
	month = jun,
	year = {2024},
	note = {arXiv:2406.09699 [math]},
	keywords = {Computer Science - Numerical Analysis, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Physics - Computational Physics, Statistics - Machine Learning},
	file = {Preprint PDF:/Users/victor/Zotero/storage/WG2KRL7V/Sapienza et al. - 2024 - Differentiable Programming for Differential Equations A Review.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/ZP6KF62Z/2406.html:text/html},
}

@inproceedings{shah_numeric_2023,
	address = {Toronto, Canada},
	title = {Numeric {Magnitude} {Comparison} {Effects} in {Large} {Language} {Models}},
	url = {https://aclanthology.org/2023.findings-acl.383},
	doi = {10.18653/v1/2023.findings-acl.383},
	urldate = {2025-07-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Raj and Marupudi, Vijay and Koenen, Reba and Bhardwaj, Khushi and Varma, Sashank},
	year = {2023},
	pages = {6147--6161},
	file = {Full Text:/Users/victor/Zotero/storage/TTSTPHT2/Shah et al. - 2023 - Numeric Magnitude Comparison Effects in Large Language Models.pdf:application/pdf},
}

@misc{jin_time_2024,
	title = {Time {Series} {Forecasting} with {LLMs}: {Understanding} and {Enhancing} {Model} {Capabilities}},
	shorttitle = {Time {Series} {Forecasting} with {LLMs}},
	url = {http://arxiv.org/abs/2402.10835},
	doi = {10.48550/arXiv.2402.10835},
	abstract = {Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of LLMs in time series forecasting under different conditions.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Jin, Mingyu and Tang, Hua and Zhang, Chong and Yu, Qinkai and Liu, Chengzhi and Zhu, Suiyuan and Zhang, Yongfeng and Du, Mengnan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.10835 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/victor/Zotero/storage/F3P5ADCT/Jin et al. - 2024 - Time Series Forecasting with LLMs Understanding and Enhancing Model Capabilities.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/XUWHR2M3/2402.html:text/html},
}

@misc{tan_are_2024,
	title = {Are {Language} {Models} {Actually} {Useful} for {Time} {Series} {Forecasting}?},
	url = {http://arxiv.org/abs/2406.16964},
	doi = {10.48550/arXiv.2406.16964},
	abstract = {Large language models (LLMs) are being applied to time series forecasting. But are language models actually useful for time series? In a series of ablation studies on three recent and popular LLM-based time series forecasting methods, we find that removing the LLM component or replacing it with a basic attention layer does not degrade forecasting performance -- in most cases, the results even improve! We also find that despite their significant computational cost, pretrained LLMs do no better than models trained from scratch, do not represent the sequential dependencies in time series, and do not assist in few-shot settings. Additionally, we explore time series encoders and find that patching and attention structures perform similarly to LLM-based forecasters.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Tan, Mingtian and Merrill, Mike A. and Gupta, Vinayak and Althoff, Tim and Hartvigsen, Thomas},
	month = oct,
	year = {2024},
	note = {arXiv:2406.16964 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted to NeurIPS 2024 (Spotlight)},
	file = {Full Text PDF:/Users/victor/Zotero/storage/3ZRNMD4V/Tan et al. - 2024 - Are Language Models Actually Useful for Time Series Forecasting.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/MWUCSVNI/2406.html:text/html},
}

@misc{jin_time-llm_2024,
	title = {Time-{LLM}: {Time} {Series} {Forecasting} by {Reprogramming} {Large} {Language} {Models}},
	shorttitle = {Time-{LLM}},
	url = {http://arxiv.org/abs/2310.01728},
	doi = {10.48550/arXiv.2310.01728},
	abstract = {Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y. and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},
	month = jan,
	year = {2024},
	note = {arXiv:2310.01728 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Accepted by the 12th International Conference on Learning Representations (ICLR 2024)},
	file = {Full Text PDF:/Users/victor/Zotero/storage/RZ232GQ8/Jin et al. - 2024 - Time-LLM Time Series Forecasting by Reprogramming Large Language Models.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/GFNZAARS/2310.html:text/html},
}

@misc{noauthor_time_2025,
	title = {Time series forecasting with {LLM}-based foundation models and scalable {AIOps} on {AWS} {\textbar} {Artificial} {Intelligence}},
	url = {https://aws.amazon.com/blogs/machine-learning/time-series-forecasting-with-llm-based-foundation-models-and-scalable-aiops-on-aws/},
	urldate = {2025-07-08},
	month = mar,
	year = {2025},
	note = {Section: Amazon SageMaker AI},
	file = {Snapshot:/Users/victor/Zotero/storage/XPMXZ6QG/time-series-forecasting-with-llm-based-foundation-models-and-scalable-aiops-on-aws.html:text/html},
}

@article{chen_locally_2011,
	title = {Locally linear embedding: a survey},
	volume = {36},
	issn = {1573-7462},
	shorttitle = {Locally linear embedding},
	url = {https://doi.org/10.1007/s10462-010-9200-z},
	doi = {10.1007/s10462-010-9200-z},
	abstract = {As a classic method of nonlinear dimensional reduction, locally linear embedding (LLE) is more and more attractive to researchers due to its ability to deal with large amounts of high dimensional data and its non-iterative way of finding the embeddings. However, several problems in the LLE algorithm still remain open, such as its sensitivity to noise, inevitable ill-conditioned eigenproblems, the lack of how to deal with the novel data, etc. The existing extensions are comprehensively reviewed and discussed classifying into different categories in this paper. Their strategies, advantages/disadvantages and performances are elaborated. By generalizing different tactics in various extensions related to different stages of LLE and evaluating their performances, several promising directions for future research have been suggested.},
	language = {en},
	number = {1},
	urldate = {2025-07-08},
	journal = {Artificial Intelligence Review},
	author = {Chen, Jing and Liu, Yang},
	month = jun,
	year = {2011},
	keywords = {Learning algorithms, Linear Algebra, Linear Logic, Linear Models and Regression, Locally linear embedding, Machine Learning, Nonlinear dimensionality reduction, Statistical Learning},
	pages = {29--48},
}

@misc{gilpin_williamgilpindysts_data_2025,
	title = {williamgilpin/dysts\_data},
	url = {https://github.com/williamgilpin/dysts_data},
	abstract = {Data files for the main dysts repository},
	urldate = {2025-07-08},
	author = {Gilpin, William},
	month = may,
	year = {2025},
	note = {original-date: 2023-12-18T22:04:59Z},
}

@misc{noauthor_gilpinlabdysts_2025,
	title = {{GilpinLab}/dysts},
	copyright = {Apache-2.0},
	url = {https://github.com/GilpinLab/dysts},
	abstract = {Hundreds of strange attractors},
	urldate = {2025-07-08},
	publisher = {GilpinLab},
	month = jul,
	year = {2025},
	note = {original-date: 2020-08-13T11:56:03Z},
}

@misc{li_revisiting_2023,
	title = {Revisiting {Long}-term {Time} {Series} {Forecasting}: {An} {Investigation} on {Linear} {Mapping}},
	shorttitle = {Revisiting {Long}-term {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2305.10721},
	doi = {10.48550/arXiv.2305.10721},
	abstract = {Long-term time series forecasting has gained significant attention in recent years. While there are various specialized designs for capturing temporal dependency, previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures. In this paper, we thoroughly investigate the intrinsic effectiveness of recent approaches and make three key observations: 1) linear mapping is critical to prior long-term time series forecasting efforts; 2) RevIN (reversible normalization) and CI (Channel Independent) play a vital role in improving overall forecasting performance; and 3) linear mapping can effectively capture periodic features in time series and has robustness for different periods across channels when increasing the input horizon. We provide theoretical and experimental explanations to support our findings and also discuss the limitations and future works. Our framework's code is available at {\textbackslash}url\{https://github.com/plumprc/RTSF\}.},
	urldate = {2025-07-08},
	publisher = {arXiv},
	author = {Li, Zhe and Qi, Shiyi and Li, Yiduo and Xu, Zenglin},
	month = may,
	year = {2023},
	note = {arXiv:2305.10721 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 12 pages, 11 figures},
	file = {Full Text PDF:/Users/victor/Zotero/storage/KN7WR423/Li et al. - 2023 - Revisiting Long-term Time Series Forecasting An Investigation on Linear Mapping.pdf:application/pdf;Snapshot:/Users/victor/Zotero/storage/67ZQXSYQ/2305.html:text/html},
}

@misc{noauthor_hmoe_nodate,
	title = {{HMoE}: {Heterogeneous} {Mixture} of {Experts} for {Language} {Modeling}},
	url = {https://arxiv.org/html/2408.10681v1},
	urldate = {2025-07-09},
	file = {HMoE\: Heterogeneous Mixture of Experts for Language Modeling:/Users/victor/Zotero/storage/WKYFUPY8/2408.html:text/html},
}
